{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### Libraries\n#### Importing Libraries","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:23.896955Z","iopub.execute_input":"2024-01-30T16:45:23.897412Z","iopub.status.idle":"2024-01-30T16:45:26.419222Z","shell.execute_reply.started":"2024-01-30T16:45:23.897375Z","shell.execute_reply":"2024-01-30T16:45:26.418181Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n#### Load the dataset","metadata":{}},{"cell_type":"code","source":"columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None, names=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:26.424952Z","iopub.execute_input":"2024-01-30T16:45:26.427691Z","iopub.status.idle":"2024-01-30T16:45:37.419658Z","shell.execute_reply.started":"2024-01-30T16:45:26.427650Z","shell.execute_reply":"2024-01-30T16:45:37.412335Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Explore the dataset","metadata":{}},{"cell_type":"code","source":"\nprint(\"First few rows of the dataset:\")\ndisplay(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:37.431341Z","iopub.execute_input":"2024-01-30T16:45:37.436435Z","iopub.status.idle":"2024-01-30T16:45:37.500917Z","shell.execute_reply.started":"2024-01-30T16:45:37.436380Z","shell.execute_reply":"2024-01-30T16:45:37.500133Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"First few rows of the dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   target         ids                          date      flag  \\\n0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  \n3          ElleCTF    my whole body feels itchy and like its on fire   \n4           Karoli  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>ids</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Last few rows of the dataset:\")\ndisplay(df.tail())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:37.505941Z","iopub.execute_input":"2024-01-30T16:45:37.508296Z","iopub.status.idle":"2024-01-30T16:45:37.526653Z","shell.execute_reply.started":"2024-01-30T16:45:37.508260Z","shell.execute_reply":"2024-01-30T16:45:37.525852Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Last few rows of the dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         target         ids                          date      flag  \\\n1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n\n                    user                                               text  \n1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>ids</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1599995</th>\n      <td>4</td>\n      <td>2193601966</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>AmandaMarie1028</td>\n      <td>Just woke up. Having no school is the best fee...</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>4</td>\n      <td>2193601969</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>TheWDBoards</td>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>4</td>\n      <td>2193601991</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>bpbabe</td>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>4</td>\n      <td>2193602064</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>tinydiamondz</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>4</td>\n      <td>2193602129</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>RyanTrevMorris</td>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Check basic information about the DataFrame","metadata":{}},{"cell_type":"code","source":"print(\"\\nInformation about the dataset:\")\ndisplay(df.info())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:37.530803Z","iopub.execute_input":"2024-01-30T16:45:37.533096Z","iopub.status.idle":"2024-01-30T16:45:37.927552Z","shell.execute_reply.started":"2024-01-30T16:45:37.533058Z","shell.execute_reply":"2024-01-30T16:45:37.926622Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nInformation about the dataset:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1600000 entries, 0 to 1599999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype \n---  ------  --------------    ----- \n 0   target  1600000 non-null  int64 \n 1   ids     1600000 non-null  int64 \n 2   date    1600000 non-null  object\n 3   flag    1600000 non-null  object\n 4   user    1600000 non-null  object\n 5   text    1600000 non-null  object\ndtypes: int64(2), object(4)\nmemory usage: 73.2+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"markdown","source":"\n#### Shape of the DataFrame","metadata":{}},{"cell_type":"code","source":"print(\"Shape of the DataFrame:\")\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:37.932120Z","iopub.execute_input":"2024-01-30T16:45:37.935079Z","iopub.status.idle":"2024-01-30T16:45:37.943941Z","shell.execute_reply.started":"2024-01-30T16:45:37.935020Z","shell.execute_reply":"2024-01-30T16:45:37.942935Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Shape of the DataFrame:\n(1600000, 6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Check for missing values","metadata":{}},{"cell_type":"code","source":"\nprint(\"Missing values in the dataset:\")\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:37.946124Z","iopub.execute_input":"2024-01-30T16:45:37.948021Z","iopub.status.idle":"2024-01-30T16:45:38.286677Z","shell.execute_reply.started":"2024-01-30T16:45:37.947984Z","shell.execute_reply":"2024-01-30T16:45:38.285883Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Missing values in the dataset:\ntarget    0\nids       0\ndate      0\nflag      0\nuser      0\ntext      0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n#### Explore the distribution of the target variable","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\nDistribution of the 'target' column:\")\nprint(df['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:38.290825Z","iopub.execute_input":"2024-01-30T16:45:38.293343Z","iopub.status.idle":"2024-01-30T16:45:38.319752Z","shell.execute_reply.started":"2024-01-30T16:45:38.293307Z","shell.execute_reply":"2024-01-30T16:45:38.318803Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nDistribution of the 'target' column:\ntarget\n0    800000\n4    800000\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Visualize the positive and negative text","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n\npositive_text = ' '.join(df[df['target'] == 4]['text'])\nnegative_text = ' '.join(df[df['target'] == 0]['text'])\n\n\nwordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\nwordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(wordcloud_positive, interpolation='bilinear')\nplt.title('Word Cloud for Positive Sentiments')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(wordcloud_negative, interpolation='bilinear')\nplt.title('Word Cloud for Negative Sentiments')\nplt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-30T16:45:38.324187Z","iopub.execute_input":"2024-01-30T16:45:38.326622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Explore the 'text' column","metadata":{}},{"cell_type":"code","source":"print(\"\\nSample text from the 'text' column:\")\ndisplay(df['text'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf['text_length'] = df['text'].apply(len)\nprint(\"\\nStatistics of text lengths:\")\nprint(df['text_length'].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Explore the length of the text","metadata":{}},{"cell_type":"markdown","source":"##### Visualize the distribution of text lengths","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(10, 6))\nsns.histplot(df['text_length'], bins=30, kde=True)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visualization of text length with and without noise","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nnoise = np.random.normal(0, 5, df.shape[0])  \ndf['noisy_text_length'] = df['text_length'] + noise\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(df['text_length'], bins=30, kde=True, color='blue')\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\n\nplt.subplot(1, 2, 2)\nsns.histplot(df['noisy_text_length'], bins=30, kde=True, color='orange')\nplt.title('Distribution of Noisy Text Lengths')\nplt.xlabel('Noisy Text Length')\nplt.ylabel('Count')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Visualize the presence of duplicate text","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(10, 6))\nsns.countplot(x=df.duplicated(subset='text'), palette='viridis')\nplt.title('Duplicate Texts Distribution')\nplt.xlabel('Duplicate Texts')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check unique target","metadata":{}},{"cell_type":"code","source":"print(\"\\nClass distribution:\")\nprint(df['target'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Plot the class distribution","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(8, 6))\nsns.countplot(x='target', data=df, palette='Set1')\nplt.title('Class Distribution')\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.xticks(ticks=[0, 1], labels=['0-Negative', '4-Positive'])  # Set custom x-axis labels\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Visualize the relationship between text length and target","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(10, 8))\nsns.boxplot(x='target', y='text_length', data=df)\nplt.title('Relationship between Sentiment and Text Length')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Identify and display pairs of duplicate text","metadata":{}},{"cell_type":"code","source":"duplicate_pairs = df[df.duplicated(subset='text', keep=False)]\nduplicate_pairs = duplicate_pairs.sort_values(by='text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Display the pairs of duplicate text","metadata":{}},{"cell_type":"code","source":"print(\"Pairs of Duplicate Text:\")\ndisplay(duplicate_pairs[['text']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop the duplicate text","metadata":{}},{"cell_type":"code","source":"df_deduplicated = df.drop_duplicates()\ndf_deduplicated_text = df.drop_duplicates(subset='text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Print the shape of the original and deduplicated DataFrames","metadata":{}},{"cell_type":"code","source":"print(\"Original DataFrame Shape:\", df.shape)\nprint(\"Deduplicated DataFrame Shape:\", df_deduplicated.shape)\ndf_deduplicated_text['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(8, 8))\nax = sns.countplot(x='target', data=df_deduplicated_text, palette='Set1')\nplt.title('Class Distribution')\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.xticks(ticks=[0, 1], labels=['0-Negative', '4-Positive'])  # Set custom x-axis labels\n#\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n#### Drop unnecessary columns","metadata":{}},{"cell_type":"code","source":"df = df[['target', 'text']]\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle missing values (if any)","metadata":{}},{"cell_type":"code","source":"\ndf.dropna(inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Text cleaning and preprocessing","metadata":{}},{"cell_type":"code","source":"\n\nimport re\n\nstop_words = set([\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n    'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n    's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn',\n    'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'\n])\n# Custom lemmatization dictionary for positive and negative words\ncustom_lemmatization_rules = {\n    'amazing': 'amaze',\n    'better': 'good',\n    'excellent': 'excel',\n    'fantastic': 'fantastic',\n    'good': 'good',\n    'great': 'great',\n    'happy': 'happy',\n    'love': 'love',\n    'positive': 'positive',\n    'wonderful': 'wonderful',\n    'splendid': 'splendid',\n    'superb': 'superb',\n    'marvelous': 'marvelous',\n    'outstanding': 'outstanding',\n    'terrific': 'terrific',\n    'delightful': 'delightful',\n    'amazing': 'amazing',\n    'magnificent': 'magnificent',\n    'joyous': 'joyous',\n    'impressive': 'impressive',\n    'fabulous': 'fabulous',\n\n    'awful': 'awful',\n    'bad': 'bad',\n    'disappointing': 'disappoint',\n    'horrible': 'horrible',\n    'negative': 'negative',\n    'poor': 'poor',\n    'terrible': 'terrible',\n    'unhappy': 'unhappy',\n    'terrible': 'terrible',\n    'miserable': 'miserable',\n    'sad': 'sad',\n    'gloomy': 'gloomy',\n    'unsatisfactory': 'unsatisfactory',\n    'dreadful': 'dreadful',\n    'unpleasant': 'unpleasant',\n    'distressing': 'distress',\n    'disheartening': 'dishearten',\n    'frustrating': 'frustrate',\n    'annoying': 'annoy',\n    'irritating': 'irritate',\n    'regrettable': 'regret',\n    'negative': 'negative',\n    'unpleasant': 'unpleasant',\n    'dismal': 'dismal',\n    'depressing': 'depress',\n    'displeasing': 'displease',\n    'disgusting': 'disgust',\n   \n}\n\n# Lemmatization function\ndef custom_lemmatize(word):\n    return custom_lemmatization_rules.get(word, word)\n\n\ndef clean_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n    # Remove common punctuation as delimiters\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Numerical character removal\n    text = re.sub('[0-9]+', '', text)\n    \n    # Remove extra whitespaces\n    text = ' '.join(text.split())\n\n    # Removing user@ references and #\n    text = re.sub(r'\\@\\w+|\\#\\w+', '', text)\n\n    # Tokenization (simple split)\n    filtered_list = text.split()\n\n        # Stopword removal\n    filtered_list = [word for word in filtered_list if word not in stop_words]\n    \n    # Lemmatization\n    filtered_list = [custom_lemmatize(word) for word in filtered_list]\n\n    cleaned_text = \" \".join(filtered_list)\n    \n    \n    return cleaned_text\n\ndirty_text = \"Hello!   This is an excellent good better example text with some 123 special characters and a URL http://example.com\"\ncleaned_text = clean_text(dirty_text)\n\nprint(\"Original Text:\", dirty_text)\nprint(\"Cleaned and Lemmatized Text:\", cleaned_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_deduplicated_text['cleaned_text'] = df_deduplicated_text['text'].apply(clean_text)\ndisplay(df_deduplicated_text['cleaned_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding the target ","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\ndf_deduplicated_text['target_encoded'] = label_encoder.fit_transform(df_deduplicated_text['target'])\ndf_deduplicated_text['target_encoded']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_deduplicated_text['target_encoded'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get sample dataset from 1.6 milliol dataset","metadata":{}},{"cell_type":"code","source":"\nmax_sample_size = 1000000\nsubset_df = df_deduplicated_text.groupby('target').apply(lambda x: x.sample(n=max_sample_size // 2)).reset_index(drop=True)\nsubset_df['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split the subset DataFrame into training and testing sets","metadata":{}},{"cell_type":"code","source":"\n\nmodel = make_pipeline(TfidfVectorizer())\n\nparam_grid = {\n    'tfidfvectorizer__max_features': [85,90,95,100,110,120],  \n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(subset_df['cleaned_text'], subset_df['target_encoded'], test_size=0.3, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_ = grid_search.best_params_\nprint(\"Best Hyperparameters :\", best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_max_features = best_params_['tfidfvectorizer__max_features']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=best_max_features,min_df=2, max_df=0.8)\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes Classifier","metadata":{}},{"cell_type":"markdown","source":"#### Hyperparameter tuning for Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n\nparam_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0,5,10,15,20,25,30, 35,40,44,45,50,55,60,65,75,100,110,115,120]}\nnb_classifier_tuned = GridSearchCV(MultinomialNB(), param_grid, cv=5)\nnb_classifier_tuned.fit(X_train_tfidf, y_train)\n\n# Best hyperparameter\nbest_alpha = nb_classifier_tuned.best_params_['alpha']\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Implimentation with best parameters","metadata":{}},{"cell_type":"code","source":"nb_classifier = MultinomialNB(alpha=best_alpha)\nnb_classifier.fit(X_train_tfidf, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Cross-validation","metadata":{}},{"cell_type":"code","source":"\nnb_cross_val_scores = cross_val_score(nb_classifier, X_train_tfidf, y_train, cv=5)\nprint('Naive Bayes Cross-Validation Scores:', nb_cross_val_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean Accuracy:\", nb_cross_val_scores.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predict on the train set","metadata":{}},{"cell_type":"code","source":"\nnb_predictions_train = nb_classifier.predict(X_train_tfidf)\ntrain_accuracy = accuracy_score(y_train,nb_predictions_train)\ntrain_accuracy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predict on the test set","metadata":{}},{"cell_type":"code","source":"\nnb_predictions = nb_classifier.predict(X_test_tfidf)\ntest_accuracy = accuracy_score(y_test,nb_predictions)\ntest_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluate the model","metadata":{}},{"cell_type":"code","source":"\nnb_accuracy = accuracy_score(y_test, nb_predictions)\nnb_classification_report = classification_report(y_test, nb_predictions)\nnb_confusion_matrix = confusion_matrix(y_test, nb_predictions)\nnb_confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot confusion marix","metadata":{}},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_test, nb_predictions)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Display results","metadata":{}},{"cell_type":"code","source":"\n\nprint(f'Naive Bayes Accuracy: {nb_accuracy:.2f}')\nprint('Naive Bayes Classification Report:')\nprint(nb_classification_report)\nprint('Naive Bayes Confusion Matrix:')\nprint(nb_confusion_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}